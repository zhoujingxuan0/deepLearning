# downsize  
`x_2 = torch.nn.fuctional.interpolate(x, scale_factor=0.5)# 将张量的最后一维缩小到原来的一半`  
# BasicConv  
```python
class BasicConv(nn.Module):
    def __init__(self, in_channel, out_channel, kernel_size, stride, bias=True, norm=False, relu=True, transpose=False):
        super(BasicConv, self).__init__()
        if bias and norm:
            bias = False

        padding = kernel_size // 2
        layers = list()
        if transpose:
            padding = kernel_size // 2 -1
            layers.append(nn.ConvTranspose2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))
        else:
            layers.append(
                nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))
        if norm:
            layers.append(nn.BatchNorm2d(out_channel))
        if relu:
            layers.append(nn.ReLU(inplace=True))# inplace=True表示修改原张量，而不是新创建一个张量
        self.main = nn.Sequential(*layers)# *表示将可迭代的参数分解成独立参数

    def forward(self, x):
        return self.main(x)
```  
# ResBlock  
```python
class ResBlock(nn.Module):
    def __init__(self, in_channel, out_channel):
        super(ResBlock, self).__init__()
        self.main = nn.Sequential(
            BasicConv(in_channel, out_channel, kernel_size=3, stride=1, relu=True),
            BasicConv(out_channel, out_channel, kernel_size=3, stride=1, relu=False)
        )

    def forward(self, x):
        return self.main(x) + x
```  
# SCM  
```python  
class SCM(nn.Module):
    def __init__(self, out_plane):
        super(SCM, self).__init__()
        self.main = nn.Sequential(
            BasicConv(3, out_plane//4, kernel_size=3, stride=1, relu=True),
            BasicConv(out_plane // 4, out_plane // 2, kernel_size=1, stride=1, relu=True),
            BasicConv(out_plane // 2, out_plane // 2, kernel_size=3, stride=1, relu=True),
            BasicConv(out_plane // 2, out_plane-3, kernel_size=1, stride=1, relu=True)
        )

        self.conv = BasicConv(out_plane, out_plane, kernel_size=1, stride=1, relu=False)
    #网络结构图【a】
    def forward(self, x):
        x = torch.cat([x, self.main(x)], dim=1)
        return self.conv(x)
```  
# EB  
```python  
class EBlock(nn.Module):
    def __init__(self, out_channel, num_res=8):
        super(EBlock, self).__init__()

        layers = [ResBlock(out_channel, out_channel) for _ in range(num_res)]

        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        return self.layers(x)

self.Encoder = nn.ModuleList([
    EBlock(base_channel, num_res),
    EBlock(base_channel*2, num_res),
    EBlock(base_channel*4, num_res),
])
```  
# DB  
```python  
class DBlock(nn.Module):
    def __init__(self, channel, num_res=8):
        super(DBlock, self).__init__()

        layers = [ResBlock(channel, channel) for _ in range(num_res)]
        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        return self.layers(x)

self.Decoder = nn.ModuleList([
    DBlock(base_channel * 4, num_res),
    DBlock(base_channel * 2, num_res),
    DBlock(base_channel, num_res)
])
```  
# feat_extract  
```python  
self.feat_extract = nn.ModuleList([
    BasicConv(3, base_channel, kernel_size=3, relu=True, stride=1),
    BasicConv(base_channel, base_channel*2, kernel_size=3, relu=True, stride=2),
    BasicConv(base_channel*2, base_channel*4, kernel_size=3, relu=True, stride=2),
    BasicConv(base_channel*4, base_channel*2, kernel_size=4, relu=True, stride=2, transpose=True),
    BasicConv(base_channel*2, base_channel, kernel_size=4, relu=True, stride=2, transpose=True),
    BasicConv(base_channel, 3, kernel_size=3, relu=False, stride=1)
]) 
```  
#  FAM(feature attention merge)  
```python  
class FAM(nn.Module):
    def __init__(self, channel):
        super(FAM, self).__init__()
        self.merge = BasicConv(channel, channel, kernel_size=3, stride=1, relu=False)

    def forward(self, x1, x2):
        x = x1 * x2 # 逐元素相乘
        out = x1 + self.merge(x)
        return out
```  
# AFF(asymmetric feature fusion)  
```python  
class AFF(nn.Module):
    def __init__(self, in_channel, out_channel):
        super(AFF, self).__init__()
        self.conv = nn.Sequential(
            BasicConv(in_channel, out_channel, kernel_size=1, stride=1, relu=True),
            BasicConv(out_channel, out_channel, kernel_size=3, stride=1, relu=False)
        )

    def forward(self, x1, x2, x4):
        x = torch.cat([x1, x2, x4], dim=1)
        return self.conv(x)

self.AFFs = nn.ModuleList([
    AFF(base_channel * 7, base_channel*1),
    AFF(base_channel * 7, base_channel*2)
])
```  
# convs  
```python  
self.Convs = nn.ModuleList([
    BasicConv(base_channel * 4, base_channel * 2, kernel_size=1, relu=True, stride=1),
    BasicConv(base_channel * 2, base_channel, kernel_size=1, relu=True, stride=1),
])
```  
# convs_out  
```python  
self.ConvsOut = nn.ModuleList(
    [
        BasicConv(base_channel * 4, 3, kernel_size=3, relu=False, stride=1),
        BasicConv(base_channel * 2, 3, kernel_size=3, relu=False, stride=1),
    ]
)
```  
